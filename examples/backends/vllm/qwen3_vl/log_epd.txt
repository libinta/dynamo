[2m2026-02-05T07:03:08.425704Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Initializing KV store discovery backend
[2m2026-02-05T07:03:08.425806Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Initializing NetworkManager with NATS request plane [3mmode[0m[2m=[0mnats
[W205 07:03:08.766212155 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
[W205 07:03:08.766385199 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
[2m2026-02-05T07:03:08.429872Z[0m [32m INFO[0m [2mdynamo_llm::http::service::service_v2[0m[2m:[0m Starting HTTP(S) service [3mprotocol[0m[2m=[0m"HTTP" [3maddress[0m[2m=[0m"0.0.0.0:8001"
[2m2026-02-05T07:03:08.440882Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_processor
[2m2026-02-05T07:03:08.440971Z[0m [32m INFO[0m [2mdynamo_llm::http::service::service_v2[0m[2m:[0m chat endpoints enabled
[W205 07:03:08.948703177 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
[2m2026-02-05T07:03:08.669239Z[0m [32m INFO[0m [2mdynamo_llm::discovery::watcher[0m[2m:[0m Chat completions is ready
[2m2026-02-05T07:03:08.687742Z[0m [32m INFO[0m [2mdynamo_llm::discovery::watcher[0m[2m:[0m added model [3mmodel_name[0m[2m=[0m"/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct/snapshots/0cfaf48183f594c314753d30a4c4974bc75f3ccb/" [3mnamespace[0m[2m=[0m"dynamo"
2026-02-05 07:03:11,882 - dynamo.nixl_connect - WARNING - dynamo.nixl_connect: Failed to load CuPy for GPU acceleration, utilizing numpy to provide CPU based operations.
2026-02-05 07:03:11,922 - dynamo.nixl_connect - WARNING - dynamo.nixl_connect: Failed to load CuPy for GPU acceleration, utilizing numpy to provide CPU based operations.
2026-02-05 07:03:11,944 - dynamo.vllm.multimodal_handlers.encode_worker_handler - WARNING - Failed to import cupy, falling back to numpy: No module named 'cupy'.
WARNING 02-05 07:03:11 [_logger.py:68] Found duplicate keys --max-model-len
[2m2026-02-05T07:03:11.968899Z[0m [32m INFO[0m [2margs.parse_args[0m[2m:[0m Setting --distributed-executor-backend=mp for TP=1 to avoid UniProcExecutor GIL contention with NIXL connector
[2m2026-02-05T07:03:11.969186Z[0m [32m INFO[0m [2margs.create_kv_transfer_config[0m[2m:[0m Creating kv_transfer_config from --connector ['nixl']
[2m2026-02-05T07:03:11.969262Z[0m [32m INFO[0m [2margs.create_kv_events_config[0m[2m:[0m Using env-var DYN_VLLM_KV_EVENT_PORT=20080 to create kv_events_config
[2m2026-02-05T07:03:11.969301Z[0m [32m INFO[0m [2margs.overwrite_args[0m[2m:[0m Using kv_events_config for publishing vLLM kv events over zmq: KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:20080', replay_endpoint=None, buffer_steps=10000, hwm=100000, max_queue_size=100000, topic='') (use_kv_events=True)
[2m2026-02-05T07:03:11.976203Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Initializing KV store discovery backend
[2m2026-02-05T07:03:11.976306Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Initializing NetworkManager with NATS request plane [3mmode[0m[2m=[0mnats
2026-02-05 07:03:11,986 - dynamo.vllm.multimodal_handlers.encode_worker_handler - WARNING - Failed to import cupy, falling back to numpy: No module named 'cupy'.
WARNING 02-05 07:03:12 [_logger.py:68] Found duplicate keys --max-model-len
[2m2026-02-05T07:03:12.010970Z[0m [32m INFO[0m [2margs.parse_args[0m[2m:[0m Setting --distributed-executor-backend=mp for TP=1 to avoid UniProcExecutor GIL contention with NIXL connector
[2m2026-02-05T07:03:12.011238Z[0m [32m INFO[0m [2margs.create_kv_transfer_config[0m[2m:[0m Creating kv_transfer_config from --connector ['nixl']
[2m2026-02-05T07:03:12.011323Z[0m [32m INFO[0m [2margs.create_kv_events_config[0m[2m:[0m Using env-var DYN_VLLM_KV_EVENT_PORT=20080 to create kv_events_config
[2m2026-02-05T07:03:12.011371Z[0m [32m INFO[0m [2margs.overwrite_args[0m[2m:[0m Using kv_events_config for publishing vLLM kv events over zmq: KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:20080', replay_endpoint=None, buffer_steps=10000, hwm=100000, max_queue_size=100000, topic='') (use_kv_events=True)
[2m2026-02-05T07:03:12.018593Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Initializing KV store discovery backend
[2m2026-02-05T07:03:12.018703Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Initializing NetworkManager with NATS request plane [3mmode[0m[2m=[0mnats
2026-02-05 07:03:12,088 - dynamo.nixl_connect - WARNING - dynamo.nixl_connect: Failed to load CuPy for GPU acceleration, utilizing numpy to provide CPU based operations.
[2m2026-02-05T07:03:12.123833Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_encoder
[2m2026-02-05T07:03:12.123897Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_backend
2026-02-05 07:03:12,150 - dynamo.vllm.multimodal_handlers.encode_worker_handler - WARNING - Failed to import cupy, falling back to numpy: No module named 'cupy'.
[2m2026-02-05T07:03:12.163405Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_backend
WARNING 02-05 07:03:12 [_logger.py:68] Found PROMETHEUS_MULTIPROC_DIR was set by user. This directory must be wiped between vLLM runs or you will find inaccurate metrics. Unset the variable and vLLM will properly handle cleanup.
INFO 02-05 07:03:12 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration
WARNING 02-05 07:03:12 [_logger.py:68] Casting torch.bfloat16 to torch.float16.
INFO 02-05 07:03:12 [model.py:1545] Using max model len 8192
[2m2026-02-05T07:03:12.173654Z[0m [32m INFO[0m [2margs.parse_args[0m[2m:[0m Setting --distributed-executor-backend=mp for TP=1 to avoid UniProcExecutor GIL contention with NIXL connector
[2m2026-02-05T07:03:12.174178Z[0m [32m INFO[0m [2margs.create_kv_transfer_config[0m[2m:[0m Creating kv_transfer_config from --connector ['nixl']
[2m2026-02-05T07:03:12.174255Z[0m [32m INFO[0m [2margs.create_kv_events_config[0m[2m:[0m Using env-var DYN_VLLM_KV_EVENT_PORT=20080 to create kv_events_config
[2m2026-02-05T07:03:12.174295Z[0m [32m INFO[0m [2margs.overwrite_args[0m[2m:[0m Using kv_events_config for publishing vLLM kv events over zmq: KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:20080', replay_endpoint=None, buffer_steps=10000, hwm=100000, max_queue_size=100000, topic='') (use_kv_events=True)
WARNING 02-05 07:03:12 [logger.py:147] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2m2026-02-05T07:03:12.180604Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Initializing KV store discovery backend
[2m2026-02-05T07:03:12.180706Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Initializing NetworkManager with NATS request plane [3mmode[0m[2m=[0mnats
INFO 02-05 07:03:12 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration
WARNING 02-05 07:03:12 [_logger.py:68] Casting torch.bfloat16 to torch.float16.
INFO 02-05 07:03:12 [model.py:1545] Using max model len 8192
INFO 02-05 07:03:12 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 02-05 07:03:12 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 02-05 07:03:12 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 02-05 07:03:12 [_logger.py:68] Enforce eager set, overriding optimization level to -O0
WARNING 02-05 07:03:12 [_logger.py:68] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py and use --no-disable-hybrid-kv-cache-manager to start vLLM.
INFO 02-05 07:03:12 [utils.py:263] non-default args: {'convert': 'mm_encoder_only', 'max_model_len': 10, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.4, 'disable_log_stats': True, 'enforce_eager': True, 'model': '/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/'}
WARNING 02-05 07:03:12 [_logger.py:68] The global random seed is set to 0. Since VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may affect the random state of the Python process that launched vLLM.
INFO 02-05 07:03:12 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration
INFO 02-05 07:03:12 [model.py:1545] Using max model len 10
[2m2026-02-05T07:03:12.325154Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_processor
[2m2026-02-05T07:03:12.325548Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_encoder
[2m2026-02-05T07:03:12.327935Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_backend
INFO 02-05 07:03:12 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration
INFO 02-05 07:03:12 [model.py:1545] Using max model len 262144
WARNING 02-05 07:03:12 [logger.py:147] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2m2026-02-05T07:03:12.338982Z[0m [32m INFO[0m [2mmain.init_multimodal_processor[0m[2m:[0m Waiting for Encoder Worker Instances ...
INFO 02-05 07:03:12 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2560.
INFO 02-05 07:03:12 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 02-05 07:03:12 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 02-05 07:03:12 [_logger.py:68] Enforce eager set, overriding optimization level to -O0
INFO 02-05 07:03:12 [core.py:97] Initializing a V1 LLM engine (v0.14.1.dev0+gb17039bcc.d20260128) with config: model='/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/', speculative_config=None, tokenizer='/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2560], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
INFO 02-05 07:03:13 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.46.160:52105 backend=xccl
INFO 02-05 07:03:13 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
2026:02:05-07:03:13:( 1308) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)
2026:02:05-07:03:13:( 1308) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[2m2026-02-05T07:03:14.900301Z[0m [33m WARN[0m [1mhttp-request[0m: [2mdynamo_llm::http::service::service_v2[0m[2m:[0m request completed with client request error [3mstatus[0m[2m=[0m404 [3mlatency_ms[0m[2m=[0m0 [2m[3mmethod[0m[2m=[0mPOST [3muri[0m[2m=[0m/v1/chat/completions [3mversion[0m[2m=[0mHTTP/1.1[0m
[2026-02-05 07:03:14.951] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
INFO 02-05 07:03:15 [gpu_model_runner.py:3808] Starting to load model /software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/...
[2m2026-02-05T07:03:15.936035Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kv_store[0m[2m:[0m KVStoreDiscovery::list: query=AllModels, prefix=v1/mdc, bucket=v1/mdc, entries=0
[2m2026-02-05T07:03:15.936076Z[0m [32m INFO[0m [2mdynamo_llm::discovery::watcher[0m[2m:[0m removed model [3mmodel_name[0m[2m=[0m"/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-32B-Instruct/snapshots/0cfaf48183f594c314753d30a4c4974bc75f3ccb/"
[2m2026-02-05T07:03:15.936087Z[0m [32m INFO[0m [2mdynamo_llm::http::service::service_v2[0m[2m:[0m chat endpoints disabled
[W205 07:03:16.564329190 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
INFO 02-05 07:03:16 [xpu.py:102] Using backend AttentionBackendEnum.FLASH_ATTN for vit attention
INFO 02-05 07:03:16 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.51it/s]

INFO 02-05 07:03:17 [default_loader.py:291] Loading weights took 0.62 seconds
INFO 02-05 07:03:17 [gpu_model_runner.py:3905] Model loading took 0.81 GiB memory and 0.967405 seconds
INFO 02-05 07:03:17 [core.py:273] init engine (profile, create kv cache, warmup model) took 0.00 seconds
WARNING 02-05 07:03:17 [_logger.py:68] Disabling chunked prefill for model without KVCache
INFO 02-05 07:03:17 [llm.py:347] Supported tasks: ('generate',)
[2m2026-02-05T07:03:17.848548Z[0m [32m INFO[0m [2mencode_worker_handler.async_init[0m[2m:[0m Encode worker startup started.
[2m2026-02-05T07:03:17.848635Z[0m [32m INFO[0m [2mencode_worker_handler.async_init[0m[2m:[0m Encode worker startup completed.
[2m2026-02-05T07:03:17.848655Z[0m [32m INFO[0m [2mmain.init_multimodal_encode_worker[0m[2m:[0m Waiting for PD Worker Instances ...
[2m2026-02-05T07:03:19.465823Z[0m [32m INFO[0m [2mcore.__init__[0m[2m:[0m Initializing a V1 LLM engine (v0.14.1.dev0+gb17039bcc.d20260128) with config: model='/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/', speculative_config=None, tokenizer='/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2m2026-02-05T07:03:19.466049Z[0m [33m WARN[0m [2m_logger.warning[0m[2m:[0m Reducing Torch parallelism from 28 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-02-05 07:03:21.569] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[W205 07:03:22.075126881 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
  _C._set_float32_matmul_precision(precision)
[2m2026-02-05T07:03:26.449722Z[0m [32m INFO[0m [2mparallel_state.init_distributed_environment[0m[2m:[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:58029 backend=xccl
[2m2026-02-05T07:03:26.457070Z[0m [32m INFO[0m [2mparallel_state.initialize_model_parallel[0m[2m:[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
2026:02:05-07:03:26:( 2633) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)
2026:02:05-07:03:26:( 2633) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[2m2026-02-05T07:03:29.102212Z[0m [32m INFO[0m [2mgpu_model_runner.load_model[0m[2m:[0m Starting to load model /software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/...
[2m2026-02-05T07:03:29.760694Z[0m [32m INFO[0m [2mxpu.get_vit_attn_backend[0m[2m:[0m Using backend AttentionBackendEnum.FLASH_ATTN for vit attention
[2m2026-02-05T07:03:29.761341Z[0m [32m INFO[0m [2mmm_encoder_attention.__init__[0m[2m:[0m Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2m2026-02-05T07:03:29.826832Z[0m [32m INFO[0m [2mxpu.get_attn_backend_cls[0m[2m:[0m Setting VLLM_KV_CACHE_LAYOUT to 'NHD' for XPU; only NHD layout is supported by XPU attention kernels.
[2m2026-02-05T07:03:29.826873Z[0m [32m INFO[0m [2mxpu.get_attn_backend_cls[0m[2m:[0m Using Flash Attention backend.
[0;36m(Worker pid=2633)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[0;36m(Worker pid=2633)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.74s/it]
[0;36m(Worker pid=2633)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.28s/it]
[0;36m(Worker pid=2633)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:06<00:00,  3.20s/it]
[0;36m(Worker pid=2633)[0;0m 
[2m2026-02-05T07:03:36.489642Z[0m [32m INFO[0m [2mdefault_loader.load_weights[0m[2m:[0m Loading weights took 6.48 seconds
[2m2026-02-05T07:03:36.939247Z[0m [32m INFO[0m [2mgpu_model_runner.load_model[0m[2m:[0m Model loading took 8.59 GiB memory and 7.050552 seconds
[2m2026-02-05T07:03:37.036683Z[0m [32m INFO[0m [2mxpu_worker.determine_available_memory[0m[2m:[0m Before memory profiling run, total GPU memory: 23256.00 MB, model load takes 8807.96 MB, free gpu memory is 317.87 MB.
[2m2026-02-05T07:03:37.036737Z[0m [32m INFO[0m [2mgpu_model_runner.profile_run[0m[2m:[0m Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[2m2026-02-05T07:03:58.137870Z[0m [32m INFO[0m [2mxpu_worker.determine_available_memory[0m[2m:[0m After memory profiling run, peak memory usage is 10911.96 MB,torch mem is 8807.96 MB, non-torch mem is 128.00 MB, free gpu memory is 14320.04 MB.
[2m2026-02-05T07:03:58.138946Z[0m [32m INFO[0m [2mkv_cache_utils._report_kv_cache_config[0m[2m:[0m GPU KV cache size: 62,912 tokens
[2m2026-02-05T07:03:58.139023Z[0m [32m INFO[0m [2mkv_cache_utils._report_kv_cache_config[0m[2m:[0m Maximum concurrency for 8,192 tokens per request: 7.68x
[2m2026-02-05T07:03:58.149987Z[0m [32m INFO[0m [2mnixl_connector[0m[2m:[0m NIXL is available
[2m2026-02-05T07:03:58.152012Z[0m [32m INFO[0m [2mfactory.create_connector[0m[2m:[0m Creating v1 connector with name: NixlConnector and engine_id: 306808bd-25b9-48a2-8a91-99a4e3c25678
[2m2026-02-05T07:03:58.152054Z[0m [33m WARN[0m [2m_logger.warning[0m[2m:[0m Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[2m2026-02-05T07:03:58.152082Z[0m [32m INFO[0m [2mnixl_connector.__init__[0m[2m:[0m Initializing NIXL wrapper
[2m2026-02-05T07:03:58.152102Z[0m [32m INFO[0m [2mnixl_connector.__init__[0m[2m:[0m Initializing NIXL worker 306808bd-25b9-48a2-8a91-99a4e3c25678
[0;36m(Worker pid=2633)[0;0m 2026-02-05 07:03:58 NIXL INFO    _api.py:361 Backend UCX was instantiated
[0;36m(Worker pid=2633)[0;0m 2026-02-05 07:03:58 NIXL INFO    _api.py:251 Initialized NIXL agent: a66832a9-a0bd-4b09-927b-7c9d4c22d737
[2m2026-02-05T07:03:58.898487Z[0m [32m INFO[0m [2mutils.get_kv_cache_layout[0m[2m:[0m `_KV_CACHE_LAYOUT_OVERRIDE` variable detected. Setting KV cache layout to NHD.
[2m2026-02-05T07:03:59.009676Z[0m [32m INFO[0m [2mnixl_connector.initialize_host_xfer_buffer[0m[2m:[0m 'enable_permute_local_kv' flag is enabled while device KV Layout is NHD. Init host buffer with HND to better support Decode/Prefill TP_ratio > 1.
[2m2026-02-05T07:03:59.010058Z[0m [32m INFO[0m [2mnixl_connector.register_kv_caches[0m[2m:[0m Registering KV_Caches. use_mla: False, kv_buffer_device: cpu, use_host_buffer: True
[2m2026-02-05T07:03:59.122244Z[0m [32m INFO[0m [2mcore._initialize_kv_caches[0m[2m:[0m init engine (profile, create kv cache, warmup model) took 22.18 seconds
[2m2026-02-05T07:03:59.510147Z[0m [32m INFO[0m [2mnixl_connector[0m[2m:[0m NIXL is available
[2m2026-02-05T07:03:59.512177Z[0m [32m INFO[0m [2mfactory.create_connector[0m[2m:[0m Creating v1 connector with name: NixlConnector and engine_id: 306808bd-25b9-48a2-8a91-99a4e3c25678
[2m2026-02-05T07:03:59.512216Z[0m [33m WARN[0m [2m_logger.warning[0m[2m:[0m Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[2m2026-02-05T07:03:59.512262Z[0m [32m INFO[0m [2mnixl_connector.__init__[0m[2m:[0m Initializing NIXL Scheduler 306808bd-25b9-48a2-8a91-99a4e3c25678
[2m2026-02-05T07:03:59.512733Z[0m [32m INFO[0m [2mkv_events.__init__[0m[2m:[0m Starting ZMQ publisher thread
[2m2026-02-05T07:04:01.874393Z[0m [32m INFO[0m [2mvllm.__post_init__[0m[2m:[0m Asynchronous scheduling is enabled.
[2m2026-02-05T07:04:01.874455Z[0m [33m WARN[0m [2m_logger.warning[0m[2m:[0m Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
INFO 02-05 07:04:02 [nixl_connector.py:97] NIXL is available
[2m2026-02-05T07:04:02.004611Z[0m [32m INFO[0m [2mmain.setup_vllm_engine[0m[2m:[0m VllmWorker for /software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/ has been initialized
INFO 02-05 07:04:02 [model.py:530] Resolved architecture: Qwen3VLForConditionalGeneration
WARNING 02-05 07:04:02 [_logger.py:68] Casting torch.bfloat16 to torch.float16.
INFO 02-05 07:04:02 [model.py:1545] Using max model len 8192
[2m2026-02-05T07:04:02.012540Z[0m [32m INFO[0m [2mengine_monitor.__init__[0m[2m:[0m VllmEngineMonitor initialized and health check task started.
[2m2026-02-05T07:04:02.012589Z[0m [32m INFO[0m [2mworker_handler.__init__[0m[2m:[0m Multimodal PD Worker startup started.
[2m2026-02-05T07:04:02.012613Z[0m [32m INFO[0m [2mworker_handler.__init__[0m[2m:[0m Multimodal PD Worker has been initialized
[2m2026-02-05T07:04:02.012674Z[0m [32m INFO[0m [2mworker_handler.async_init[0m[2m:[0m Multimodal PD Worker async initialization completed.
[2m2026-02-05T07:04:02.012701Z[0m [32m INFO[0m [2mmain.setup_kv_event_publisher[0m[2m:[0m KV event publisher for dp_rank=0 subscribing to vLLM at tcp://127.0.0.1:20080
[2m2026-02-05T07:04:02.012737Z[0m [32m INFO[0m [2mdynamo_llm::kv_router::publisher[0m[2m:[0m Initializing KvEventPublisher for worker 7587892662608322828 in component backend
[2m2026-02-05T07:04:02.012795Z[0m [32m INFO[0m [2mmain.setup_kv_event_publisher[0m[2m:[0m Worker reading KV events for dp_rank=0 from tcp://127.0.0.1:20080
[2m2026-02-05T07:04:02.015760Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Creating NATS request plane server
[2m2026-02-05T07:04:02.015785Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m NatsMultiplexedServer::register_endpoint called [3mendpoint_name[0m[2m=[0mgenerate [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mbackend [3minstance_id[0m[2m=[0m7587892662608322828
[2m2026-02-05T07:04:02.015795Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Successfully retrieved service group
[2m2026-02-05T07:04:02.015811Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Registering NATS endpoint [3mendpoint_name[0m[2m=[0mgenerate [3mendpoint_with_id[0m[2m=[0mgenerate-694d9c2c9c8fe10c [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mbackend [3minstance_id[0m[2m=[0m7587892662608322828
[2m2026-02-05T07:04:02.015816Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Starting NATS push endpoint listener (blocking) [3mendpoint_name[0m[2m=[0mgenerate [3mendpoint_with_id[0m[2m=[0mgenerate-694d9c2c9c8fe10c
[2m2026-02-05T07:04:02.015882Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m NatsMultiplexedServer::register_endpoint called [3mendpoint_name[0m[2m=[0mclear_kv_blocks [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mbackend [3minstance_id[0m[2m=[0m7587892662608322828
[2m2026-02-05T07:04:02.015917Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Successfully retrieved service group
[2m2026-02-05T07:04:02.015933Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Registering NATS endpoint [3mendpoint_name[0m[2m=[0mclear_kv_blocks [3mendpoint_with_id[0m[2m=[0mclear_kv_blocks-694d9c2c9c8fe10c [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mbackend [3minstance_id[0m[2m=[0m7587892662608322828
[2m2026-02-05T07:04:02.015938Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Starting NATS push endpoint listener (blocking) [3mendpoint_name[0m[2m=[0mclear_kv_blocks [3mendpoint_with_id[0m[2m=[0mclear_kv_blocks-694d9c2c9c8fe10c
[2m2026-02-05T07:04:02.027977Z[0m [32m INFO[0m [2mdynamo_runtime::component::client[0m[2m:[0m wait_for_instances: Found 1 instance(s) for endpoint: dynamo/backend/generate
[2m2026-02-05T07:04:02.028194Z[0m [32m INFO[0m [2mmain.init_multimodal_encode_worker[0m[2m:[0m Starting to serve the encode worker endpoint...
[2m2026-02-05T07:04:02.028821Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Creating NATS request plane server
[2m2026-02-05T07:04:02.028837Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m NatsMultiplexedServer::register_endpoint called [3mendpoint_name[0m[2m=[0mgenerate [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mencoder [3minstance_id[0m[2m=[0m7587892662608322826
[2m2026-02-05T07:04:02.028847Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Successfully retrieved service group
[2m2026-02-05T07:04:02.028862Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Registering NATS endpoint [3mendpoint_name[0m[2m=[0mgenerate [3mendpoint_with_id[0m[2m=[0mgenerate-694d9c2c9c8fe10a [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mencoder [3minstance_id[0m[2m=[0m7587892662608322826
[2m2026-02-05T07:04:02.028869Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Starting NATS push endpoint listener (blocking) [3mendpoint_name[0m[2m=[0mgenerate [3mendpoint_with_id[0m[2m=[0mgenerate-694d9c2c9c8fe10a
[2m2026-02-05T07:04:02.040587Z[0m [32m INFO[0m [2mdynamo_runtime::component::client[0m[2m:[0m wait_for_instances: Found 1 instance(s) for endpoint: dynamo/encoder/generate
[2m2026-02-05T07:04:02.045090Z[0m [32m INFO[0m [2m_core[0m[2m:[0m Registered base model '/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/' MDC
[2m2026-02-05T07:04:02.045259Z[0m [32m INFO[0m [2mmain.init_multimodal_processor[0m[2m:[0m Starting to serve the processor endpoint...
[2m2026-02-05T07:04:02.045737Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Creating NATS request plane server
[2m2026-02-05T07:04:02.045750Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m NatsMultiplexedServer::register_endpoint called [3mendpoint_name[0m[2m=[0mgenerate [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mprocessor [3minstance_id[0m[2m=[0m7587892662608322832
[2m2026-02-05T07:04:02.045759Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Successfully retrieved service group
[2m2026-02-05T07:04:02.045772Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Registering NATS endpoint [3mendpoint_name[0m[2m=[0mgenerate [3mendpoint_with_id[0m[2m=[0mgenerate-694d9c2c9c8fe110 [3mnamespace[0m[2m=[0mdynamo [3mcomponent[0m[2m=[0mprocessor [3minstance_id[0m[2m=[0m7587892662608322832
[2m2026-02-05T07:04:02.045776Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::nats_server[0m[2m:[0m Starting NATS push endpoint listener (blocking) [3mendpoint_name[0m[2m=[0mgenerate [3mendpoint_with_id[0m[2m=[0mgenerate-694d9c2c9c8fe110
[2m2026-02-05T07:04:02.209636Z[0m [32m INFO[0m [2mdynamo_llm::http::service::service_v2[0m[2m:[0m chat endpoints enabled
[2m2026-02-05T07:04:02.243475Z[0m [32m INFO[0m [2mdynamo_llm::discovery::watcher[0m[2m:[0m Chat completions is ready
[2m2026-02-05T07:04:02.260958Z[0m [32m INFO[0m [2mdynamo_llm::discovery::watcher[0m[2m:[0m added model [3mmodel_name[0m[2m=[0m"/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-4B-Instruct/snapshots/ebb281ec70b05090aa6165b016eac8ec08e71b17/" [3mnamespace[0m[2m=[0m"dynamo"
[2m2026-02-05T07:08:08.219825Z[0m [32m INFO[0m [2mhttp_client.get_http_client[0m[2m:[0m Shared HTTP client initialized with timeout=30.0s
[2m2026-02-05T07:08:08.960593Z[0m [32m INFO[0m [2mworker_handler.generate[0m[2m:[0m PD: Loading local safetensors file
[2m2026-02-05T07:08:08.963583Z[0m [32m INFO[0m [2mworker_handler.generate[0m[2m:[0m Prepared multimodal data size: 2
[2m2026-02-05T07:08:08.965936Z[0m [32m INFO[0m [2mworker_handler.generate[0m[2m:[0m defaultdict(<class 'list'>, {'image': {'image_embeds': tensor([[ 0.4238, -0.5117,  0.9062,  ...,  0.5273,  0.3633,  0.1318],
        [ 6.7812, -1.6016,  0.5938,  ...,  2.0938,  0.1416,  0.0435],
        [ 3.6250, -0.1357,  0.2617,  ...,  0.0129, -0.0234, -0.2578],
        ...,
        [ 4.8125, -1.3125,  1.6094,  ...,  0.3516,  0.0415,  0.0801],
        [ 5.1562, -1.7969,  1.0234,  ...,  0.1245, -0.1787,  0.1816],
        [ 2.1094, -0.5742,  1.2656,  ...,  1.1250,  0.4043, -0.2061]],
       dtype=torch.float16), 'image_grid_thw': tensor([[ 1, 30, 40]])}})
[2m2026-02-05T07:09:09.049400Z[0m [32m INFO[0m [2mworker_handler.generate[0m[2m:[0m PD: Loading local safetensors file
[2m2026-02-05T07:09:09.050173Z[0m [32m INFO[0m [2mworker_handler.generate[0m[2m:[0m Prepared multimodal data size: 2
[2m2026-02-05T07:09:09.051185Z[0m [32m INFO[0m [2mworker_handler.generate[0m[2m:[0m defaultdict(<class 'list'>, {'image': {'image_embeds': tensor([[ 0.4238, -0.5117,  0.9062,  ...,  0.5273,  0.3633,  0.1318],
        [ 6.7812, -1.6016,  0.5938,  ...,  2.0938,  0.1416,  0.0435],
        [ 3.6250, -0.1357,  0.2617,  ...,  0.0129, -0.0234, -0.2578],
        ...,
        [ 4.8125, -1.3125,  1.6094,  ...,  0.3516,  0.0415,  0.0801],
        [ 5.1562, -1.7969,  1.0234,  ...,  0.1245, -0.1787,  0.1816],
        [ 2.1094, -0.5742,  1.2656,  ...,  1.1250,  0.4043, -0.2061]],
       dtype=torch.float16), 'image_grid_thw': tensor([[ 1, 30, 40]])}})
