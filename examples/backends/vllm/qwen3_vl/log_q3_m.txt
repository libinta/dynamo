[W205 18:28:32.842476885 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
2026-02-05 18:28:36,899 - dynamo.nixl_connect - WARNING - dynamo.nixl_connect: Failed to load CuPy for GPU acceleration, utilizing numpy to provide CPU based operations.
2026-02-05 18:28:36,972 - dynamo.vllm.multimodal_handlers.encode_worker_handler - WARNING - Failed to import cupy, falling back to numpy: No module named 'cupy'.
[2m2026-02-05T18:28:36.998393Z[0m [32m INFO[0m [2margs.create_kv_transfer_config[0m[2m:[0m Using vLLM defaults for kv_transfer_config
[2m2026-02-05T18:28:36.998449Z[0m [32m INFO[0m [2margs.create_kv_events_config[0m[2m:[0m Using env-var DYN_VLLM_KV_EVENT_PORT=20080 to create kv_events_config
[2m2026-02-05T18:28:36.998496Z[0m [32m INFO[0m [2margs.overwrite_args[0m[2m:[0m Using kv_events_config for publishing vLLM kv events over zmq: KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:20080', replay_endpoint=None, buffer_steps=10000, hwm=100000, max_queue_size=100000, topic='') (use_kv_events=True)
[2m2026-02-05T18:28:37.005345Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Initializing KV store discovery backend
[2m2026-02-05T18:28:37.005439Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Initializing NetworkManager with NATS request plane [3mmode[0m[2m=[0mnats
[2m2026-02-05T18:28:37.148416Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Added NATS service dynamo_backend
WARNING 02-05 18:28:37 [_logger.py:68] Found PROMETHEUS_MULTIPROC_DIR was set by user. This directory must be wiped between vLLM runs or you will find inaccurate metrics. Unset the variable and vLLM will properly handle cleanup.
INFO 02-05 18:28:37 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
WARNING 02-05 18:28:37 [_logger.py:68] Casting torch.bfloat16 to torch.float16.
INFO 02-05 18:28:37 [model.py:1545] Using max model len 8192
WARNING 02-05 18:28:37 [logger.py:147] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 02-05 18:28:37 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
WARNING 02-05 18:28:37 [_logger.py:68] Casting torch.bfloat16 to torch.float16.
INFO 02-05 18:28:37 [model.py:1545] Using max model len 8192
INFO 02-05 18:28:37 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 02-05 18:28:37 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 02-05 18:28:37 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 02-05 18:28:37 [_logger.py:68] Enforce eager set, overriding optimization level to -O0
[2026-02-05 18:28:40.367] [warning] [sycl_collector.h:388] Another subscriber already subscribed to Sycl runtime events, so PTI will not subscribe to them. It will affect correctness of PTI profile: e.g. report zero XPU time for CPU callers of GPU kernels.
[W205 18:28:41.151267739 OperatorEntry.cpp:218] Warning: Warning only once for all operators,  other operators may also be overridden.
  Overriding a previously registered kernel for the same operator and the same dispatch key
  operator: aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -> Tensor(a!)
    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6
  dispatch key: XPU
  previous kernel: registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:37
       new kernel: registered at /root/workspace/frameworks.ai.pytorch.ipex-gpu/build/Release/csrc/gpu/csrc/gpu/xpu/ATen/RegisterXPU_0.cpp:172 (function operator())
[2m2026-02-05T18:28:45.073543Z[0m [32m INFO[0m [2mcore.__init__[0m[2m:[0m Initializing a V1 LLM engine (v0.14.1.dev0+gb17039bcc.d20260128) with config: model='/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct-FP8/', speculative_config=None, tokenizer='/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct-FP8/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=fp8, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=xpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct-FP8/, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=4977)[0;0m /usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   _C._set_float32_matmul_precision(precision)
[2m2026-02-05T18:28:45.759600Z[0m [32m INFO[0m [2mparallel_state.init_distributed_environment[0m[2m:[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.26.46.160:36453 backend=xccl
[2m2026-02-05T18:28:45.793315Z[0m [32m INFO[0m [2mparallel_state.initialize_model_parallel[0m[2m:[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
2026:02:05-18:28:45:( 4977) |CCL_WARN| value of CCL_ATL_TRANSPORT changed to be ofi (default:mpi)
2026:02:05-18:28:45:( 4977) |CCL_WARN| could not get local_idx/count from environment variables, trying to get them from ATL
[2m2026-02-05T18:28:48.639649Z[0m [32m INFO[0m [2mgpu_model_runner.load_model[0m[2m:[0m Starting to load model /software/data/pytorch/huggingface/hub/models--Qwen--Qwen3-VL-30B-A3B-Instruct-FP8/...
[2m2026-02-05T18:28:49.321491Z[0m [32m INFO[0m [2mxpu.get_vit_attn_backend[0m[2m:[0m Using backend AttentionBackendEnum.FLASH_ATTN for vit attention
[2m2026-02-05T18:28:49.324385Z[0m [32m INFO[0m [2mmm_encoder_attention.__init__[0m[2m:[0m Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2m2026-02-05T18:28:49.635168Z[0m [32m INFO[0m [2mxpu.get_attn_backend_cls[0m[2m:[0m Setting VLLM_KV_CACHE_LAYOUT to 'NHD' for XPU; only NHD layout is supported by XPU attention kernels.
[2m2026-02-05T18:28:49.635211Z[0m [32m INFO[0m [2mxpu.get_attn_backend_cls[0m[2m:[0m Using Flash Attention backend.
[2m2026-02-05T18:28:49.640323Z[0m [32m INFO[0m [2mfp8.select_fp8_moe_backend[0m[2m:[0m DeepGEMM is disabled because the platform does not support it.
[2m2026-02-05T18:28:49.640369Z[0m [32m INFO[0m [2mfp8.select_fp8_moe_backend[0m[2m:[0m Using Triton backend for FP8 MoE
[2m2026-02-05T18:28:50.057106Z[0m [31mERROR[0m [2mcore.run_engine_core[0m[2m:[0m EngineCore failed to start.
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
    engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 692, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 106, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__
    self._init_executor()
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
    self.driver_worker.load_model()
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
    self.model_runner.load_model(eep_scale_up=eep_scale_up)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3827, in load_model
    self.model = model_loader.load_model(
                 ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
    model = initialize_model(
            ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
    return model_class(vllm_config=vllm_config, prefix=prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 440, in __init__
    self.language_model = Qwen3MoeLLMForCausalLM(
                          ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 337, in __init__
    self.model = Qwen3MoeLLMModel(
                 ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 305, in __init__
    old_init(self, **kwargs)
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 86, in __init__
    super().__init__(vllm_config=vllm_config, prefix=prefix)
  File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 305, in __init__
    old_init(self, **kwargs)
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 412, in __init__
    self.start_layer, self.end_layer, self.layers = make_layers(
                                                    ^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 606, in make_layers
    maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 414, in <lambda>
    lambda prefix: Qwen3MoeDecoderLayer(vllm_config=vllm_config, prefix=prefix),
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 352, in __init__
    self.mlp = Qwen3MoeSparseMoeBlock(
               ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 163, in __init__
    self.experts = FusedMoE(
                   ^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 620, in __init__
    self.quant_method: FusedMoEMethodBase = _get_quant_method()
                                            ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 612, in _get_quant_method
    quant_method = self.quant_config.get_quant_method(self, prefix)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 221, in get_quant_method
    return self.get_xpu_quant_method(layer, prefix)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 212, in get_xpu_quant_method
    return XPUFp8MoEMethod(fp8_config, layer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/ipex_quant.py", line 337, in __init__
    super().__init__(quant_config, layer)
  File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 1090, in __init__
    assert not quant_config.is_checkpoint_fp8_serialized
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError
[0;36m(EngineCore_DP0 pid=4977)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=4977)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 940, in run_engine_core
[0;36m(EngineCore_DP0 pid=4977)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[0;36m(EngineCore_DP0 pid=4977)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[0;36m(EngineCore_DP0 pid=4977)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 692, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     super().__init__(
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py", line 106, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.model_executor = executor_class(vllm_config)
[0;36m(EngineCore_DP0 pid=4977)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self._init_executor()
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.driver_worker.load_model()
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py", line 3827, in load_model
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.model = model_loader.load_model(
[0;36m(EngineCore_DP0 pid=4977)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py", line 50, in load_model
[0;36m(EngineCore_DP0 pid=4977)[0;0m     model = initialize_model(
[0;36m(EngineCore_DP0 pid=4977)[0;0m             ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
[0;36m(EngineCore_DP0 pid=4977)[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=4977)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 440, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.language_model = Qwen3MoeLLMForCausalLM(
[0;36m(EngineCore_DP0 pid=4977)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 337, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.model = Qwen3MoeLLMModel(
[0;36m(EngineCore_DP0 pid=4977)[0;0m                  ^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 305, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 86, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     super().__init__(vllm_config=vllm_config, prefix=prefix)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py", line 305, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     old_init(self, **kwargs)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 412, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(
[0;36m(EngineCore_DP0 pid=4977)[0;0m                                                     ^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py", line 606, in make_layers
[0;36m(EngineCore_DP0 pid=4977)[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f"{prefix}.{idx}"))
[0;36m(EngineCore_DP0 pid=4977)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 414, in <lambda>
[0;36m(EngineCore_DP0 pid=4977)[0;0m     lambda prefix: Qwen3MoeDecoderLayer(vllm_config=vllm_config, prefix=prefix),
[0;36m(EngineCore_DP0 pid=4977)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 352, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.mlp = Qwen3MoeSparseMoeBlock(
[0;36m(EngineCore_DP0 pid=4977)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3_moe.py", line 163, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.experts = FusedMoE(
[0;36m(EngineCore_DP0 pid=4977)[0;0m                    ^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 620, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     self.quant_method: FusedMoEMethodBase = _get_quant_method()
[0;36m(EngineCore_DP0 pid=4977)[0;0m                                             ^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/layer.py", line 612, in _get_quant_method
[0;36m(EngineCore_DP0 pid=4977)[0;0m     quant_method = self.quant_config.get_quant_method(self, prefix)
[0;36m(EngineCore_DP0 pid=4977)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 221, in get_quant_method
[0;36m(EngineCore_DP0 pid=4977)[0;0m     return self.get_xpu_quant_method(layer, prefix)
[0;36m(EngineCore_DP0 pid=4977)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 212, in get_xpu_quant_method
[0;36m(EngineCore_DP0 pid=4977)[0;0m     return XPUFp8MoEMethod(fp8_config, layer)
[0;36m(EngineCore_DP0 pid=4977)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/ipex_quant.py", line 337, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     super().__init__(quant_config, layer)
[0;36m(EngineCore_DP0 pid=4977)[0;0m   File "/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/fp8.py", line 1090, in __init__
[0;36m(EngineCore_DP0 pid=4977)[0;0m     assert not quant_config.is_checkpoint_fp8_serialized
[0;36m(EngineCore_DP0 pid=4977)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=4977)[0;0m AssertionError
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.12/dist-packages/dynamo/vllm/__main__.py", line 12, in <module>
    main()
  File "/usr/local/lib/python3.12/dist-packages/dynamo/vllm/main.py", line 1048, in main
    uvloop.run(worker())
  File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 96, in run
    return __asyncio.run(
           ^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/dynamo/vllm/main.py", line 152, in worker
    await init(runtime, config, shutdown_event)
  File "/usr/local/lib/python3.12/dist-packages/dynamo/vllm/main.py", line 562, in init
    ) = setup_vllm_engine(config, factory)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/dynamo/vllm/main.py", line 354, in setup_vllm_engine
    engine_client = AsyncLLM.from_vllm_config(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/async_llm.py", line 132, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 824, in __init__
    super().__init__(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py", line 479, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 921, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
